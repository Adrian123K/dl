{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 수치미분</b>\n",
    "    진정한 미분은 컴퓨터로 구현할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T05:02:45.716586Z",
     "start_time": "2020-08-05T05:02:45.711589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-50\n",
    "    return (f(x+h)-f(x))/h\n",
    "\n",
    "print(np.float32(1e-50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    컴퓨터로 구현하면 분모가 0이 되어 계산이 안된다.\n",
    "    계산되는 근사로 구한 접선으로 수학식을 구현하면\n",
    "$$\\lim_{h->0}{{f(x+h)-f(x-h)}\\over {(x+h)-(x-h)}} \\, = \\lim_{h->0}{{f(x+h)-f(x-h)}\\over {2h}} $$\n",
    "\n",
    "    위의 식을 파이썬으로 구현하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T05:06:15.673666Z",
     "start_time": "2020-08-05T05:06:15.669670Z"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제50. $f(x)=2x^{2}$함수의 $x$가 $3$에서의 미분계수(기울기)를 구하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T05:07:22.972867Z",
     "start_time": "2020-08-05T05:07:22.959874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.000000000025324"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 2*x**2\n",
    "\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "numerical_diff(f,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제51. 아래의 함수를 생성하고 $x$가 $2$에서의 미분계수(기울기)를 구하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T05:15:33.783166Z",
     "start_time": "2020-08-05T05:15:33.776169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.00000026014936"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 3*x**4 + 2*x**3 + 6*x**2 + 7\n",
    "\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "numerical_diff(f,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 편미분 (p.125)</b>\n",
    "$$z=x^{2}+y^{2}$$\n",
    "\n",
    "    변수가 2개 이상인 함수를 미분할 때 미분 대상 변수외에 나머지 변수를 상수처럼 고정시켜 미분하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제52. 손으로 아래의 함수를 편미분하시오\n",
    "$$f(x_{0},x_{1})=x_{0}^2+x_{1}^2$$\n",
    "<center><b> $x_{0}=3, \\, x_{1}=4$ 일 때 , $x_{0}$에 대해 편미분하시오</b></center>\n",
    "\n",
    "$${{\\partial f}\\over {\\partial x_{0}}} = 2x_{0}, \\; {{\\partial f}\\over {\\partial x_{1}}} = 2x_{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제53. 아래의 수학식의 오차함수를 생성하시오\n",
    "$$f(x_{0},x_{1})=x_{0}^2+x_{1}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T05:37:47.698955Z",
     "start_time": "2020-08-05T05:37:47.679966Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([3.0, 4.0])\n",
    "\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "def loss_func(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "loss_func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제54. 위의 loss_func() 함수를 $x_{0}=3, \\; x_{1}=4$ 에서 $x_{0}$에서 편미분 했을 때의 기울기는?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T06:06:41.319160Z",
     "start_time": "2020-08-05T06:06:41.309165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([3.0, 4.0])\n",
    "\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h)-f(x-h))/(2*h)\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def loss_func(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4**2\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3**2 + x1**2\n",
    "\n",
    "numerical_diff(function_tmp1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    위의 편미분 방법은 손으로 나머지 하나를 상수화 시켜서 강제로 구현한 코드이므로 파이썬으로 알아서 편미분 하도록 함수를 생성하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 편미분하는 함수 numerical_gradient 만들기</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T06:07:56.992056Z",
     "start_time": "2020-08-05T06:07:56.980062Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy  as  np\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "def loss_func(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "\n",
    "def  numerical_gradient(f,x):\n",
    "    h = 1e-04\n",
    "    grad = np.zeros_like(x) \n",
    "    \n",
    "    for  i  in  range(x.size):\n",
    "        tmp_val = x[i] \n",
    "        \n",
    "        x[i] = tmp_val + h  \n",
    "        fxh1 = f(x) \n",
    "\n",
    "        x[i] = tmp_val - h                          \n",
    "        fxh2 = f(x)  \n",
    "        \n",
    "        grad[i] = ( fxh1 - fxh2 ) / (2*h)          \n",
    "        \n",
    "        x[i] = tmp_val  \n",
    "\n",
    "    return  grad \n",
    "\n",
    "x = np.array([3.0, 4.0])\n",
    "numerical_gradient(loss_func, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제55. 위의 $i$가 0번째일 때의 디버깅 처럼 $i$가 1번째도 디버깅 하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T06:32:02.115298Z",
     "start_time": "2020-08-05T06:32:02.106302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n"
     ]
    }
   ],
   "source": [
    "import numpy  as  np\n",
    "\n",
    "def f(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "def loss_func(x):\n",
    "    return x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-04\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for i in range(x.size): # i : 1\n",
    "        tmp_val = x[i] # tmp_val : 4.0\n",
    "\n",
    "        x[i] = tmp_val + h # x[1] = 4.0001\n",
    "        fxh1 = f(x) # fxh1 : 9 + 16.0001 = 25.0008\n",
    "\n",
    "        x[i] = tmp_val - h # x1 = 3.9999\n",
    "        fxh2 = f(x) # fxh2 : 9 + 15.9992 = 24.9992\n",
    "\n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h) # grad[1] = (25.0008 - 24.9992) / 0.0002 = 8.0\n",
    "\n",
    "        x[i] = tmp_val # x[1] : 4.0\n",
    "\n",
    "    return grad\n",
    "\n",
    "x = np.array([3.0, 4.0])\n",
    "print(numerical_gradient(loss_func, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    편미분을 해서 기울기를 각각 구하는 numerical_gradient 함수를 만든 목적은 '경사하강'을 구현하기 위해서.\n",
    "    최소 오차지점으로 찾아가기 위함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 경사하강법 (p.129)</b>\n",
    "    numerical_gradient 함수가 출력하는 값은 기울기\n",
    "    \n",
    "    가중치 = 가중치 - 학습률*기울기\n",
    "    \n",
    "    위의 식을 loop문으로 계속 반복해서 수행해서 기울기가 0이 되면 가중치가 변경이 안되는 그 시점까지 수행해서 최적의 가중치를 알아낼 것\n",
    "    \n",
    "    학습률(learning rate)\n",
    "        한 번의 학습으로 얼마만큼 매개변수를 갱신할지를 결정하는 하이퍼 파라미터\n",
    "        *하이퍼 파라미터 - 신경망을 학습시키기 위해서 직접 알아내야 하는 매개변수\n",
    "        \n",
    "        학습률 값은 0.01이나 0.001 등 미리 특정 값으로 정해줘야 하는데 \n",
    "        일반적으로 이 값이 너무 크거나 너무 작으면 global minima를 찾아갈 수 없다\n",
    "        신경망 학습에서는 보통 이 학습률을 조정하면서 올바르게 학습하고 있는지 확인하며 훈련을 시킨다\n",
    "        \n",
    "        학습률이 너무 ↑ --> 학습은 빠르지만 global minima를 지나칠 수 있다\n",
    "        학습률이 너무 ↓ --> global minima를 지나칠 염려는 없지만 학습이 너무 느려서 수렴을 못할 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제56. 방금 만든 numerical_gradient 함수를 이용해서 경사하강함수인 gradient_decent 함수를 생성하시오\n",
    "    p.131 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T06:44:49.065868Z",
     "start_time": "2020-08-05T06:44:49.058872Z"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-04\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for i in range(x.size): \n",
    "        tmp_val = x[i] \n",
    "\n",
    "        x[i] = tmp_val + h \n",
    "        fxh1 = f(x) \n",
    "\n",
    "        x[i] = tmp_val - h \n",
    "        fxh2 = f(x) \n",
    "\n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h) \n",
    "\n",
    "        x[i] = tmp_val \n",
    "\n",
    "    return grad\n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제57. 함수 $f(x_{0},x_{1})=x_{0}^{2}+x_{1}^{2}$ 함수에서 [-3.0, 4.0]에서 최소 지점으로 경사하강하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T06:44:50.441442Z",
     "start_time": "2020-08-05T06:44:50.430449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 2층(단층) 신경망 구현하기</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T07:16:13.946905Z",
     "start_time": "2020-08-05T07:16:13.854960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54679998  0.03426175 -0.58106173]\n",
      " [ 0.82019997  0.05139263 -0.8715926 ]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (1,3)\n",
    "        y = softmax(z) # (1,3)\n",
    "        loss = cross_entropy_error(y, t) # t(1,3)와 y(1,3)의 오차\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t) # lambda로 w에 net.loss(x, t)결과 할당. 비용함수 생성\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(dW) # 기울기 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    가중치가 (2,3) 행렬이므로 기울기도 (2,3)행렬로 나와야 가중치 - 기울기 연산 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제58. 오차값을 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T07:24:57.335121Z",
     "start_time": "2020-08-05T07:24:57.323128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19231867 -0.59440166 -0.68207508]\n",
      "[0.53405951 0.24317636 0.22276413]\n",
      "1.5016413112735538\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (1,3)\n",
    "        y = softmax(z) # (1,3)\n",
    "        loss = cross_entropy_error(y, t) # t(1,3)와 y(1,3)의 오차\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t) # lambda로 w에 net.loss(x, t)결과 할당. 비용함수 생성\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(net.predict(x))\n",
    "print(softmax(net.predict(x)))\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제59. simpleNet 클래스의 predict 함수를 실행하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T07:29:16.978738Z",
     "start_time": "2020-08-05T07:29:16.964747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43305645,  0.01956848, -1.74143718])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (1,3)\n",
    "        y = softmax(z) # (1,3)\n",
    "        loss = cross_entropy_error(y, t) # t(1,3)와 y(1,3)의 오차\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t) # lambda로 w에 net.loss(x, t)결과 할당. 비용함수 생성\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "net.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제60. simpleNet 클래스에 있는 가중치 행렬 W를 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T07:29:11.344486Z",
     "start_time": "2020-08-05T07:29:11.331495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85488041,  0.88842495,  1.40590524],\n",
       "       [-0.2415135 , -0.3844434 ,  1.1367138 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (1,3)\n",
    "        y = softmax(z) # (1,3)\n",
    "        loss = cross_entropy_error(y, t) # t(1,3)와 y(1,3)의 오차\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t) # lambda로 w에 net.loss(x, t)결과 할당. 비용함수 생성\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "net.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제61. simpleNet 클래스의 loss 함수 의 y 변수 값은 softmax 함수를 통과한 확률벡터인데 이중에 최댓값 원소의 인덱스 번호를 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T07:33:22.532021Z",
     "start_time": "2020-08-05T07:33:22.518028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0.31799064 0.64810617 0.03390319]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (1,3)\n",
    "        y = softmax(z) # (1,3)\n",
    "        loss = cross_entropy_error(y, t) # t(1,3)와 y(1,3)의 오차\n",
    "\n",
    "        return loss\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t) # lambda로 w에 net.loss(x, t)결과 할당. 비용함수 생성\n",
    "dW = numerical_gradient(f, net.W)\n",
    "\n",
    "print(np.argmax(softmax(net.predict(x))))\n",
    "print(softmax(net.predict(x)))\n",
    "# print(np.argmax(net.loss(x, t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제62. (오늘의 마지막 문제) 책 138페이지 나오는 accuracy 함수를 simpleNet 클래스에 추가해서 정확도를 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T07:52:10.776830Z",
     "start_time": "2020-08-05T07:52:10.760841Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x) # (1,3)\n",
    "        y = softmax(z) # (1,3)\n",
    "        loss = cross_entropy_error(y, t) # t(1,3)와 y(1,3)의 오차\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        p = np.argmax(softmax(y))\n",
    "        t = np.argmax(t)\n",
    "        \n",
    "        accuracy = np.sum(p==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "t = np.array([0, 0, 1])\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "net.accuracy(x,t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
