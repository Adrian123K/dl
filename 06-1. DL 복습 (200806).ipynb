{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 복습</b>\n",
    "    1장. Numpy\n",
    "    2장. 퍼셉트론\n",
    "    3장. 신경망 (저자가 만들어온 가중치)\n",
    "    4장. 신경망 (수치미분을 이용)\n",
    "        1. 단층 신경망 (p.134)\n",
    "        2. 다층 신경망 (p.137)\n",
    "    5장. 신경망 (오차역전파 이용)\n",
    "    \n",
    "### <b>■ 단층 신경망 코드구현 (p.134)</b>\n",
    "![simple](simplelayer.png)\n",
    "\n",
    "$$T = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix} \\\\ y = \\begin{bmatrix} 0.7 & 0.2 & 0.1 \\end{bmatrix} \\\\ -\\sum_{i=1}^{n}t_{i}\\cdot \\log y_{i} : 오차함수 \\\\\\\\\n",
    "softmax = {{e^{x_{i}}} \\over {-\\sum_{i=1}^{n}e^{x_{i}}}} \\\\ \\begin{bmatrix} {{e^{k_{1}}} \\over {e^{k_{1}}+e^{k_{2}}+e^{k_{3}}}} & {{e^{k_{2}}} \\over {e^{k_{1}}+e^{k_{2}}+e^{k_{3}}}} & {{e^{k_{3}}} \\over {e^{k_{1}}+e^{k_{2}}+e^{k_{3}}}} \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import  numpy  as  np\n",
    "from common.functions  import  softmax, cross_entropy_error\n",
    "\n",
    "class  simpleNet:\n",
    "    def  __init__(self):\n",
    "        self.W = np.random.randn(2,3)  # 정규분포로 2x3 행렬의 가중치 생성\n",
    "\n",
    "    def  predict( self, x ):\n",
    "        return  np.dot( x, self.W )\n",
    "\n",
    "    def  loss( self, x, t ):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return  loss \n",
    "net = simpleNet()\n",
    "print ( net.W )  # 가중치 행렬 확인 \n",
    "\n",
    "x = np.array( [0.6, 0.9] )\n",
    "y = net.predict(x)\n",
    "print (y)\n",
    "\n",
    "print ( np.argmax(y) ) \n",
    "\n",
    "t = np.array([ 0, 0, 1])\n",
    "print ( net.loss(x,t)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제63. simpleNet 클래스의 단층 신경망 코드를 활용해서 아래의 신경망을 구현하시오\n",
    "![two](twosimplelayer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T02:09:19.952289Z",
     "start_time": "2020-08-06T02:09:19.923306Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931469805599654\n"
     ]
    }
   ],
   "source": [
    "import  sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import  numpy  as  np\n",
    "from common.functions  import  softmax, cross_entropy_error, relu\n",
    "\n",
    "class  simpleNet:\n",
    "    def  __init__(self):\n",
    "        self.W1 = np.random.randn(2,3)\n",
    "        self.W2 = np.random.randn(3,2)\n",
    "        self.W3 = np.random.randn(2,2)\n",
    "\n",
    "    def  predict( self, x ):\n",
    "        W1, W2, W3 = self.W1, self.W2, self.W3\n",
    "        s1 = np.dot(x, W1)\n",
    "        r1 = relu(s1)\n",
    "        s2 = np.dot(r1, W2)\n",
    "        r2 = relu(s2)\n",
    "        s3 = np.dot(r2, W3)\n",
    "        y = softmax(s3)\n",
    "        return  y\n",
    "    \n",
    "    def  loss( self, x, t ):\n",
    "        z = self.predict(x)        \n",
    "        loss = cross_entropy_error(z, t)\n",
    "\n",
    "        return  loss \n",
    "    \n",
    "net = simpleNet()\n",
    "\n",
    "x = np.array( [0.3, 0.8] )\n",
    "t = np.array([ 0, 1])\n",
    "\n",
    "print(net.loss(x, t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제64. 아래의 신경망을 클래스로 만들어서 구현하시오. 오차값을 출력\n",
    "![three](threelayer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T03:19:32.264726Z",
     "start_time": "2020-08-06T03:19:32.228748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4708483084087915\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error, relu\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleNet:\n",
    "    def __init__(self, input_s, hidden1_s, hidden2_s,hidden3_s, output_s):\n",
    "        self.params={}\n",
    "        self.params['W1']=np.random.randn(input_s, hidden1_s)\n",
    "        self.params['W2']=np.random.randn(hidden1_s, hidden2_s)\n",
    "        self.params['W3']=np.random.randn(hidden2_s, hidden3_s)\n",
    "        self.params['W4']=np.random.randn(hidden3_s, output_s)        \n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.params.values():\n",
    "            h = np.dot(x, layer)\n",
    "            self.mask = (h<=0)\n",
    "            h[self.mask] = 0\n",
    "        for i in self.output.values():\n",
    "            hout = np.dot(h, i)\n",
    "        y = predict(hout)\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = softmax(x)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x = np.array([0.4, 0.1, 0.8, 0.9])\n",
    "    t = np.array([0, 1])\n",
    "    net = SimpleNet(input_s=4, hidden1_s=5, hidden2_s=4, hidden3_s=5, output_s=2)\n",
    "    print(net.loss(x,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제65. (점심시간 문제) 문제 64번의 클래스에 정확도를 출력하는 함수를 추가하고 정확도를 출력하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T04:44:17.457380Z",
     "start_time": "2020-08-06T04:44:17.441389Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error, relu\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class  simpleNet:\n",
    "    def  __init__(self,input_s, hidden1_s, hidden2_s, hidden3_s, output_s):\n",
    "        self.W1 = np.random.randn(input_s,hidden1_s)\n",
    "        self.W2 = np.random.randn(hidden1_s, hidden2_s)\n",
    "        self.W3 = np.random.randn(hidden2_s, hidden3_s)\n",
    "        self.W4 = np.random.randn(hidden3_s, output_s)\n",
    "\n",
    "    def  predict( self, x ):\n",
    "        W1, W2, W3, W4 = self.W1, self.W2, self.W3, self.W4\n",
    "        s1 = np.dot(x, W1)\n",
    "        r1 = relu(s1)\n",
    "        s2 = np.dot(r1, W2)\n",
    "        r2 = relu(s2)\n",
    "        s3 = np.dot(r2, W3)\n",
    "        r3 = relu(s3)\n",
    "        s4 = np.dot(r3, W4)\n",
    "        y = softmax(s4)\n",
    "        return  y\n",
    "    \n",
    "    def  loss( self, x, t ):\n",
    "        z = self.predict(x)        \n",
    "        loss = cross_entropy_error(z, t)\n",
    "\n",
    "        return  loss\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = np.argmax(z)\n",
    "        t = np.argmax(t)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    x = np.array([0.4, 0.1, 0.8, 0.9])\n",
    "    t = np.array([0, 1])\n",
    "    net = simpleNet(4,5,4,5,2)\n",
    "    print(net.accuracy(x,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 2층 신경망을 설명하기 위해 간소하게 만든 코드</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T05:05:00.905426Z",
     "start_time": "2020-08-06T05:05:00.641589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 8 5 8 8 5 8 5 8 5 8 5 8 8 8 8 8 8 8 8 8 8 5 8 5 5 8 8 8 5 8 5 8 8 5 8\n",
      " 8 8 8 5 5 8 8 8 8 8 5 8 5 5 8 8 5 8 8 8 8 5 5 8 8 8 5 8 5 8 5 5 8 8 8 8 8\n",
      " 5 8 5 5 8 8 8 8 8 8 8 5 5 8 8 8 8 8 8 8 8 8 8 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "        load_mnist(normalize=True, flatten=True, one_hot_label=True)\n",
    "    return x_train, t_train\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = 0.01 * np.random.randn(784, 50)\n",
    "        self.params['b1'] = np.zeros(50)\n",
    "        self.params['W2'] = 0.01 * np.random.randn(50, 10)\n",
    "        self.params['b2'] = np.zeros(10)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def softmax(self, a):\n",
    "        minus = a - np.max(a)\n",
    "        return np.exp(minus) / np.sum(np.exp(minus))\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = self.softmax(z)\n",
    "        return self.cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y_hat = np.argmax(y, axis=1)\n",
    "        target = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y_hat == target) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "net = TwoLayerNet()\n",
    "x, t = get_data()\n",
    "\n",
    "y = net.predict(x[:100]) # 100장만 넣어서 예측\n",
    "y_hat = np.argmax(y, axis=1)\n",
    "print(y_hat)\n",
    "# y = net.accuracy(x[:100],t[:100])\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제66. 위의 예측한 100장에 대한 실제 정답 100개의 숫자는 무엇인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T05:05:01.015427Z",
     "start_time": "2020-08-06T05:05:01.011429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n",
      " 0 7 6 1 8 7 9 3 9 8 5 9 3 3 0 7 4 9 8 0 9 4 1 4 4 6 0 4 5 6 1 0 0 1 7 1 6\n",
      " 3 0 2 1 1 7 9 0 2 6 7 8 3 9 0 4 6 7 4 6 8 0 7 8 3 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(t[:100],axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제67. 위의 100장에 대한 정확도는 어떻게 되는가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T05:05:02.056008Z",
     "start_time": "2020-08-06T05:05:02.050011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n"
     ]
    }
   ],
   "source": [
    "print(net.accuracy(x[:100],t[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    신경망 학습을 시키려면 기울기가 필요한데 기울기를 출력하는 함수를 이용해서 학습을 시킨다.    \n",
    "        1. numerical_gradient 함수를 TwoLayerNet 클래스에 붙여 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as  np\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for i in range(x.size):\n",
    "        tmp_val = x[i]\n",
    "        x[i] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[i] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[i] = (fxh1 - fxh2) / (2 * h)\n",
    "\n",
    "        x[i] = tmp_val\n",
    "\n",
    "    return grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T05:10:55.162644Z",
     "start_time": "2020-08-06T05:10:55.155649Z"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(self, x, t):\n",
    "    loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "    grads = {}\n",
    "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T05:05:00.905426Z",
     "start_time": "2020-08-06T05:05:00.641589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 8 5 8 8 5 8 5 8 5 8 5 8 8 8 8 8 8 8 8 8 8 5 8 5 5 8 8 8 5 8 5 8 8 5 8\n",
      " 8 8 8 5 5 8 8 8 8 8 5 8 5 5 8 8 5 8 8 8 8 5 5 8 8 8 5 8 5 8 5 5 8 8 8 8 8\n",
      " 5 8 5 5 8 8 8 8 8 8 8 5 5 8 8 8 8 8 8 8 8 8 8 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "        load_mnist(normalize=True, flatten=True, one_hot_label=True)\n",
    "    return x_train, t_train\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = 0.01 * np.random.randn(784, 50)\n",
    "        self.params['b1'] = np.zeros(50)\n",
    "        self.params['W2'] = 0.01 * np.random.randn(50, 10)\n",
    "        self.params['b2'] = np.zeros(10)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def softmax(self, a):\n",
    "        minus = a - np.max(a)\n",
    "        return np.exp(minus) / np.sum(np.exp(minus))\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = self.softmax(z)\n",
    "        return self.cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y_hat = np.argmax(y, axis=1)\n",
    "        target = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y_hat == target) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "net = TwoLayerNet()\n",
    "x, t = get_data()\n",
    "# batch_size = np.random.choice(len(x),100)\n",
    "batch_size = 100\n",
    "lr = 0.01\n",
    "\n",
    "for i in range(10):\n",
    "    x_batch = x[:batch_size]\n",
    "    t_batch = t[:batch_size]\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    for key in ('W1','W2','b1','b2'):\n",
    "        net.params[key] -= lr*grad[key]\n",
    "    y = net.accuracy(x_batch, t_batch)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제68. 입력한 숫자 하나를 처음에 어떻게 예측했는지 출력하고 학습되면서 예측한 숫자가 어떻게 바뀌는지 확인하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-06T05:47:29.916323Z",
     "start_time": "2020-08-06T05:40:27.274649Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "[8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = \\\n",
    "        load_mnist(normalize=True, flatten=True, one_hot_label=True)\n",
    "    return x_train, t_train\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = 0.01 * np.random.randn(784, 50)\n",
    "        self.params['b1'] = np.zeros(50)\n",
    "        self.params['W2'] = 0.01 * np.random.randn(50, 10)\n",
    "        self.params['b2'] = np.zeros(10)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def softmax(self, a):\n",
    "        minus = a - np.max(a)\n",
    "        return np.exp(minus) / np.sum(np.exp(minus))\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        delta = 1e-7\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(t * np.log(y + delta)) / batch_size\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = self.softmax(z)\n",
    "        return self.cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y_hat = np.argmax(y, axis=1)\n",
    "        target = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y_hat == target) / float(x.shape[0])\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "net = TwoLayerNet()\n",
    "x, t = get_data()\n",
    "# batch_size = np.random.choice(len(x),100)\n",
    "batch_size = 100\n",
    "lr = 0.01\n",
    "\n",
    "for i in range(10):\n",
    "    x_batch = x[:batch_size]\n",
    "    t_batch = t[:batch_size]\n",
    "    grad = net.numerical_gradient(x_batch, t_batch)\n",
    "    for key in ('W1','W2','b1','b2'):\n",
    "        net.params[key] -= lr*grad[key]\n",
    "    y = net.predict(x_batch)\n",
    "    y_hat = np.argmax(y, axis = 1)\n",
    "    print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제69. 인터넷에서 선호하는 사진을 한 장 다운 받아서 사진 사이즈를 확인하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
