{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> ■ 복습 </b>\n",
    "    1장. Numpy\n",
    "    2장. 퍼셉트론\n",
    "    3장. 신경망 (저자가 만든 가중치로 3층 신경망 구현)\n",
    "    4장. 신경망 구현 (직접 학습시켜 2층 신경망 구현 - 수치미분)\n",
    "        수치미분으로 신경망을 학습 시키기에는 너무 시간이 많이 걸린다\n",
    "    5장. 신경망 구현 (직접 학습시켜 2층 신경망 구현 - 오차역전파)\n",
    "        신경망의 함수들의 도함수를 만들어서 역전파 함수로 구현하고 오차를 역전파해서 마지막에는 가중치 행렬(W1, W2)를 변경하게끔 구현\n",
    "         \n",
    "                 Affine1(W1)+b1                Affine2(W2)+b2\n",
    "        입력층 ------------------> 시그모이드 ------------------> 시그모이드 ------------------> 소프트맥스 ------------------> 교차 엔트로피\n",
    "                                       ↓                           ↓                            ↓                              ↓\n",
    "                                     도함수                        도함수                        도함수                           도함수\n",
    "                     <----------------------------------------------------------------------------------------------------------------------- 비용(역전파)\n",
    "\n",
    "    면접문제: 수치미분과 오차역전파의 차이가 무엇인가?\n",
    "        답: \n",
    "        \n",
    "### <b>■ 신경망안에 들어가는 함수를 클래스로 구성</b>\n",
    "    클래스 구성>                    \n",
    "    1. ReLU 함수 클래스               모든 클래스별로\n",
    "    2. Sigmoid 함수                   순전파 / 역전파    \n",
    "    3. Affine 계층                      함수를 구현\n",
    "    4. Softmax 함수\n",
    "    5. Cross Entropy\n",
    "    \n",
    "### <b>■ Softmax 함수 클래스 구현 및 Cross Entropy 함수 클래스 구현 (Softmax with Loss) </b>\n",
    "    1. 소프트맥스 함수 식\n",
    "    2. 소프트맥스 함수 계산그래프\n",
    "    3. 교차 엔트로피 함수 식\n",
    "    4. 교차 엔트로피 함수 계산 그래프\n",
    "    5. 소프트맥스 + 교차 엔트로피 묶어서 코드 구현\n",
    "![soft-cross1](http://cfile271.uf.daum.net/image/999678455F3074570A379A)\n",
    "![soft-cross2](http://cfile296.uf.daum.net/image/998D43455F3073720A1C81)\n",
    "![soft-cross3](http://cfile287.uf.daum.net/image/9908BD4A5F30748F0BFDBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제81. 책 179페이지의 SoftmaxWithLoss 클래스를 생성하고 구현하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T01:59:42.120203Z",
     "start_time": "2020-08-10T01:59:42.107211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4063467509023186\n",
      "[ 0.00901439  0.00901439 -0.09098561  0.00901439  0.00901439  0.00901439\n",
      "  0.00928892  0.01204708  0.00986332  0.01471434]\n",
      "1.5475681948007376\n",
      "[ 0.0087373   0.0087373  -0.07872354  0.0087373   0.0087373   0.0087373\n",
      "  0.0087373   0.0087373   0.0087373   0.00882511]\n"
     ]
    }
   ],
   "source": [
    "from common.functions import *\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx\n",
    "\n",
    "t = np.array([0,0,1,0,0,0,0,0,0,0])  # 숫자 2\n",
    "\n",
    "x1 = np.array([0.01,0.01,0.01,0.01,0.01,0.01,0.04,0.3,0.1,0.5])\n",
    "x2 = np.array([0.01,0.01,0.9,0.01,0.01,0.01,0.01,0.01,0.01,0.02])\n",
    "\n",
    "soft = SoftmaxWithLoss()\n",
    "\n",
    "print (soft.forward(x1,t))\n",
    "print (soft.backward()) \n",
    "\n",
    "print (soft.forward(x2,t))\n",
    "print (soft.backward() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ orderdict의 이해</b>\n",
    "    OrderedDict는 일반 Dictionary와 다르게 입력된 데이터 뿐만 아니라 입력된 순서까지 같아야 동일한 것으로 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-10T02:08:48.141454Z",
     "start_time": "2020-08-10T02:08:48.127462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dict : \n",
      "{'a': 'A', 'b': 'B', 'c': 'C', 'd': 'D', 'e': 'E'}\n",
      "{'e': 'E', 'd': 'D', 'c': 'C', 'b': 'B', 'a': 'A'}\n",
      "True\n",
      "\n",
      "\n",
      "OrderDict : \n",
      "OrderedDict([('a', 'A'), ('b', 'B'), ('c', 'C'), ('d', 'D'), ('e', 'E')])\n",
      "OrderedDict([('e', 'E'), ('d', 'D'), ('c', 'C'), ('b', 'B'), ('a', 'A')])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# 예제:  일반 딕셔너리 예제\n",
    "print (' dict : ')\n",
    "\n",
    "d1 = {}\n",
    "d1['a'] = 'A'\n",
    "d1['b'] = 'B'\n",
    "d1['c'] = 'C'\n",
    "d1['d'] = 'D'\n",
    "d1['e'] = 'E'\n",
    "\n",
    "d2 = {}\n",
    "d2['e'] = 'E'\n",
    "d2['d'] = 'D'\n",
    "d2['c'] = 'C'\n",
    "d2['b'] = 'B' \n",
    "d2['a'] = 'A'\n",
    "\n",
    "print (d1) # {'a': 'A', 'b': 'B', 'c': 'C', 'd': 'D', 'e': 'E'}\n",
    "print (d2) # {'e': 'E', 'd': 'D', 'c': 'C', 'b': 'B', 'a': 'A'}\n",
    "\n",
    "print ( d1 == d2 )  # True  \n",
    "\n",
    "# OrderDict() 를 테스트 \n",
    "from collections import OrderedDict\n",
    "\n",
    "print('\\n')\n",
    "print ('OrderDict : ')\n",
    "\n",
    "d1 = collections.OrderedDict()\n",
    "d1['a'] = 'A'\n",
    "d1['b'] = 'B'\n",
    "d1['c'] = 'C'\n",
    "d1['d'] = 'D'\n",
    "d1['e'] = 'E'\n",
    "\n",
    "d2 = collections.OrderedDict()\n",
    "d2['e'] = 'E'\n",
    "d2['d'] = 'D'\n",
    "d2['c'] = 'C'\n",
    "d2['b'] = 'B' \n",
    "d2['a'] = 'A'\n",
    "\n",
    "print (d1)\n",
    "print (d2) \n",
    "\n",
    "OrderedDict([('a', 'A'), ('b', 'B'), ('c', 'C'), ('d', 'D'), ('e', 'E')])\n",
    "OrderedDict([('e', 'E'), ('d', 'D'), ('c', 'C'), ('b', 'B'), ('a', 'A')])\n",
    "\n",
    "print (d1 == d2)  # False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    OrderDictionary는 순서까지 같아야 같은 데이터로 인식\n",
    "    \n",
    "    신경망에서는 순서가 아주 중요\n",
    "        순전파로 갔던 길 그대로 역전파가 되어야하기 때문\n",
    "    \n",
    "    순전파 : 입력값 --> Affine1 --> ReLU --> Affine2 --> ReLU --> Affine3 --> Lastlayer(SoftmaxWithLoss) --> 오차\n",
    "    역전파 : 오차(1) --> LastLayer --> Affine3 --> ReLU --> Affine2 --> ReLU --> Affine1\n",
    "    \n",
    "    ※ 순전파의 순서를 반대로(reverse)해서 역전파 될 수 있도록 OrderedDict 함수를 신경망 코드에 사용해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제1. 오차역전파를 이용한 2층 신경망 전체코드를 구현하시오\n",
    "# coding: utf-8\n",
    "import sys, os\n",
    "\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1: t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0] # 60000 개\n",
    "batch_size = 100  # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num): # 10000\n",
    "    # 미니배치 획득  # 랜덤으로 100개씩 뽑아서 10000번을 수행하니까 백만번\n",
    "    batch_mask = np.random.choice(train_size, batch_size) # 100개 씩 뽑아서 10000번 백만번\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss) # cost 가 점점 줄어드는것을 보려고\n",
    "\n",
    "    # 1에폭당 정확도 계산 # 여기는 훈련이 아니라 1에폭 되었을때 정확도만 체크\n",
    "    if i % iter_per_epoch == 0: # 600 번마다 정확도 쌓는다.\n",
    "\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc) # 10000/600 개  16개 # 정확도가 점점 올라감\n",
    "        test_acc_list.append(test_acc)  # 10000/600 개 16개 # 정확도가 점점 올라감\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
